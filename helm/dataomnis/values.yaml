# Default values for dataomnis.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# for apiserver and flink web
domain: dataomnis.192.168.27.90.nip.io
port:

# the host info for pod
# qingcloudApiIP: the ip of api.qingcloud.com
dns:
  qingcloudApiIP: 172.31.60.141

image:
  # kubectl -n dataomnis create secret docker-registry docker-registry-secret --docker-server=dockerhub.qingcloud.com --docker-username=push-dataomnis --docker-password=<your-pword>
  # if need, first create docker-registry secret named "docker-registry-secret" by above cmd in namespace dataomnis and set secret to "docker-registry-secret"
  pullSecret:
  pullPolicy: Always  # pullPolicy: IfNotPresent

  registry: dockerhub.databench.io
  tag: dev
  zeppelinTag: 0.9.0

  busybox: busybox:1.28.4
  zeppelinInterpreter: dataomnis/zeppelin:0.9.0
  jaeger: jaegertracing/all-in-one:1.22

# common configuration for all service
common:
  replicas: 1
  strategy: RollingUpdate
  logLevel: 1  # 1=>"debug", 2=>"info", 3=>"warn", 4=>"error", 5=>"fatal"
  grpcLog:
    level: 2  #  1 => info, 2 => waring, 3 => error, 4 => fatal
    verbosity: 99
  metrics:
    enabled: true
    urlPath: "/metrics"
  livenessProbe:
    initialDelaySeconds: 10
    periodSeconds: 15
  readinessProbe:
    periodSeconds: 10
    initialDelaySeconds: 20

etcd:
  endpoint: etcd-cluster

# mysql configuration
mysql:
  internal: false  # if use internal mysql in dependencies
  # common mysql configuration for dataomnis services
  externalHost: mysql-cluster-pxc-db-haproxy
  secretName: mysql-cluster-pxc-db
  user: "root"
  database: "dataomnis"
  maxIdleConn: 16
  maxOpenConn: 128
  connMaxLifetime: 10m
  logLevel: 4  # 1 => Silent, 2 => Error, 3 => Warn, 4 => Info
  slowTshreshold: 2s

hdfs:
  configmapName: hdfs-cluster-common-config

# redis-cluster configuration
redis:
  address: rfs-redis-cluster
  database: 0
  masterName: "mymaster"

# apiglobal settings
apiglobal:
  enable: true
  # region settings
  regions:
    testing:
      hosts: "http://api.dataomnis.192.168.27.90.nip.io" # apiserver export service
      names:
        zh_cn: "开发测试区"
        en_us: "testing"
  envs:
    HTTP_SERVER_READ_TIMEOUT: 30s
    HTTP_SERVER_WRITE_TIMEOUT: 30s
    HTTP_SERVER_DLE_TIMEOUT: 30s
    HTTP_SERVER_EXIT_TIMEOUT: 5m
    TRACER_SERVICE_NAME: apiglobal

# apiserver configuration
apiserver:
  envs:
    READ_TIMEOUT: 30s
    WRITE_TIMEOUT: 30s
    DLE_TIMEOUT: 30s
    EXIT_TIMEOUT: 5m
    TRACER_SERVICE_NAME: "apiserver"
    REGION_ID: "testing"

# account configuration
account:
  envs:
    TRACER_SERVICE_NAME: "account"
    SOURCE: "qingcloud"

# enginemanager configuration
enginemanager:
  replicas: 1
  helmRepoPath: "/root/.cache/helm/repository"
  kubeConfigPath: "/root/.kube/config"
  envs:
    LOG_LEVEL: 1
    GRPC_LOG_LEVEL: 1
    TRACER_SERVICE_NAME: "enginemanager"

    HELM_REPOSITORY_CONFIG: "/root/.config/helm/repositories.yaml"
    HELM_REPOSITORY_CACHE: "/root/.cache/helm/repository"
    HELM_DRIVER: ""
    HELM_DEBUG: false
    HELM_QUERY_STATUS_DURATION: 10

    FLINK_REST_SERVICE_PORT: "8081"
    FLINK_REST_SERVICE_NAME_FMT: "%s-flink-jobmanager-rest"
    FLINK_INGRESS_CLASS: "nginx"
    FLINK_ENABLE_MULTUS: false

# jobmanager configuration
jobmanager:
  envs:
    TRACER_SERVICE_NAME: "jobmanager"

# resourcemanager configuration
resourcemanager:
  envs:
    HADOOP_CONF_DIR: /etc/hadoop
    TRACER_SERVICE_NAME: "resourcemanager"

# scheduler configuration
scheduler:
  envs:
    TRACER_SERVICE_NAME: "scheduler"
    ETCD_DIAL_TIMEOUT: 5s

# spacemanager configuration
spacemanager:
  envs:
     TRACER_SERVICE_NAME: "spacemanager"
     ETCD_DIAL_TIMEOUT: 5s
     CLUSTER_FLINK_REST_SERVICE_PORT: "8081"
     CLUSTER_FLINK_REST_SERVICE_NAME_FMT: "%s-flink-jobmanager-rest"

# zeppelin configuration
zeppelin:
  hdfsConfigDir: /zeppelin/hadoop/hadoop-2.7.5/etc/hadoop
  envs:
    ZEPPELIN_HOME: /opt/zeppelin
    ZEPPELIN_SERVER_RPC_PORTRANGE: 12320:12320
    ZEPPELIN_K8S_SPARK_CONTAINER_IMAGE: spark:2.4.5
    SPARK_MASTER: k8s://https://kubernetes.default.svc
    SPARK_HOME: /spark

developer:
  envs:
    DEVELOPER_SPACE_MANAGER_SERVER_ENABLEKEEPALIVE: true
    DEVELOPER_SPACE_MANAGER_SERVER_KEEPALIVEWITHOUTCALLS: true
    DEVELOPER_SPACE_MANAGER_SERVER_NEGOTIATIONTYPE: plaintext

# jaeger configuration
jaeger:
  envs:
    COLLECTOR_ZIPKIN_HTTP_PORT: 9411

# iaas configuration
iaas:
  zone: "testing"
  protocol: "http"
  host: "api.testing.com"
  port: "7777"
  uri: "/iaas/"
  timeout: 600
  access_key_id: "LTMJGBXPHSEZRNVKKPHU"
  secret_access_key: "7GvVuGAx2iB8NA9n8NtczH8BJnTkDGwGm9N6DYBo"

# serviceName: port
ports:
  jaeger: 6831
  jaegerweb: 16686
  mysql: 3306
  etcd: 2379
  zeppelin: 8080
  hdfs: 8020
  redis: 26379  # keep same as the port of redis cluster (Sentinel-Mode)

  apiglobal: 8001
  apiserver: 9001
  spacemanager: 9101
  scheduler: 9103
  jobmanager: 9105
  account: 9110
  resourcemanager: 9111
  notifier: 9113
  enginemanager: 9114
  developer: 9119
  developerHttp: 9219

# add serviceMonitor for service(key must be same as component in deployment);
# the serviceMonitor would be auto created if add the metricsPorts of service below.
metricsPorts:
  spacemanager: 9201
  scheduler: 9203
  jobmanager: 9205
  account: 9210
  resourcemanager: 9211
  enginemanager: 9214

serviceMonitor:
  enabled: true
  defaults:
    labels:
      project: dataomnis
  namespace: kubesphere-monitoring-system
  port: metrics
  interval: 30s

webservice:
  enabled: false
