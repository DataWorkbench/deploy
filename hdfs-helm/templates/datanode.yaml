# Source: hdfs/charts/hdfs-datanode-k8s/templates/datanode-daemonset.yaml
# Provides datanode helper scripts.
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-hdfs-datanode-scripts
  labels:
    app: hdfs-datanode
    chart: hdfs-datanode-k8s-0.1.0
    release: my-hdfs-datanode
data:
  check-status.sh: |
    #!/usr/bin/env bash
    # Exit on error. Append "|| true" if you expect an error.
    set -o errexit
    # Exit on error inside any functions or subshells.
    set -o errtrace
    # Do not allow use of undefined vars. Use ${VAR:-} to use an undefined VAR
    set -o nounset
    # Catch an error in command pipes. e.g. mysqldump fails (but gzip succeeds)
    # in `mysqldump |gzip`
    set -o pipefail
    # Turn on traces, useful while debugging.
    set -o xtrace

    # Check if datanode registered with the namenode and got non-null cluster ID.
    _PORTS="50075 1006"
    _URL_PATH="jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo"
    _CLUSTER_ID=""
    for _PORT in $_PORTS; do
      _CLUSTER_ID+=$(curl -s http://localhost:${_PORT}/$_URL_PATH |  \
          grep ClusterId) || true
    done
    echo $_CLUSTER_ID | grep -q -v null
---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-hdfs-datanode
  labels:
    app: hdfs-datanode
    chart: hdfs-datanode-k8s-0.1.0
    release: my-hdfs-datanode
spec:
  selector:
    matchLabels:
      app: hdfs-datanode
  template:
    metadata:
      labels:
        app: hdfs-datanode
        release: my-hdfs-datanode
    spec:
      hostNetwork: true
      hostPID: true
      dnsPolicy: ClusterFirstWithHostNet
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: hdfs/nodetype
                    operator: In
                    values:
                      - datanode
      containers:
        - name: datanode
          image: uhopper/hadoop-datanode:2.7.2
#          resources:
#            requests:
#              memory: "12Gi"
#              cpu: "6"
          env:
            - name: HADOOP_CUSTOM_CONF_DIR
              value: /etc/hadoop-custom-conf
            - name: MULTIHOMED_NETWORK
              value: "0"
          livenessProbe:
            exec:
              command:
                - /dn-scripts/check-status.sh
            initialDelaySeconds: 60
            periodSeconds: 30
          readinessProbe:
            exec:
              command:
                - /dn-scripts/check-status.sh
            initialDelaySeconds: 60
            periodSeconds: 30
          securityContext:
            privileged: true
          volumeMounts:
            - name: dn-scripts
              mountPath: /dn-scripts
              readOnly: true
            - name: hdfs-config
              mountPath: /etc/hadoop-custom-conf
              readOnly: true
            - name: hdfs-data-0
              mountPath: /hadoop/dfs/data/0
      restartPolicy: Always
      volumes:
        - name: dn-scripts
          configMap:
            name: my-hdfs-datanode-scripts
            defaultMode: 0744
        - name: hdfs-data-0
          hostPath:
            path: /mnt/hdfs/dn-data
        - name: hdfs-config
          configMap:
            name: my-hdfs-config